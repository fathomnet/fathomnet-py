{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifmDW0dirq9D"
      },
      "source": [
        "# FathomNet Python API Tutorial\n",
        "*So you want to use FathomNet data...*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/fathomnet/fathomnet-logo/main/FathomNet_white_CenterText_400px.png\" alt=\"FathomNet logo\" width=\"200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNYBxwg3UsZe"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "> `fathomnet-py` is a client-side API to help scientists, researchers, and developers interact with FathomNet data.\n",
        "\n",
        "[![tests](https://github.com/fathomnet/fathomnet-py/actions/workflows/tests.yml/badge.svg)](https://github.com/fathomnet/fathomnet-py/actions/workflows/tests.yml)\n",
        "[![Documentation Status](https://readthedocs.org/projects/fathomnet-py/badge/?version=latest)](https://fathomnet-py.readthedocs.io/en/latest/?badge=latest)\n",
        "\n",
        "The [fathomnet-py](https://github.com/fathomnet/fathomnet-py) API offers native Python interaction with the FathomNet REST API, abstracting away the underlying HTTP requests.\n",
        "This notebook is designed to walk you through some of the core functionality of the API and to illustrate a common use case: *training an object detector*. \n",
        "\n",
        "It's split into two parts:\n",
        "1. [**The API**](#api): API overview and data visualizations\n",
        "2. [**Example use case**](#usecase): training an object detector and running inference\n",
        "\n",
        "This notebook is by no means exhaustive; it serves to show some common \"recipes\" for pulling down and handling FathomNet data in Python. **Full documentation for fathomnet-py is available at [fathomnet-py.readthedocs.io](https://fathomnet-py.readthedocs.io).**\n",
        "\n",
        "[FathomNet GitHub](https://github.com/fathomnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM6WRbgWWkV1"
      },
      "source": [
        "### Installing `fathomnet-py`\n",
        "\n",
        "To install fathomnet-py, you will need to have Python 3.7 or greater installed first (as of the time of writing, this notebook ships with Python 3.9). Then, from the command-line:\n",
        "\n",
        "```bash\n",
        "pip install fathomnet\n",
        "```\n",
        "\n",
        "This notebook installs fathomnet-py in the [Setup](#setup) section next, along with some relevant packages for data manipulation and visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkYCmUX6r0su"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uNeHCauXyvC"
      },
      "source": [
        "First, we'll install a few packages via pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q--V_Xrr3js"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U fathomnet plotly ipyleaflet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfilAbptX2LU"
      },
      "source": [
        "and import the auxiliary modules we need for part 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpcYIXG-sofq"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets                      # Provides embedded widgets\n",
        "import ipyleaflet                                 # Provides map widgets\n",
        "import requests                                   # Manages HTTP requests\n",
        "import numpy as np                                # Facilitates array/matrix operations\n",
        "import plotly.express as px                       # Generates nice plots\n",
        "import random                                     # Generates pseudo-random numbers\n",
        "from PIL import Image, ImageFont, ImageDraw       # Facilitates image operations\n",
        "from io import BytesIO                            # Interfaces byte data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iazt9ae-sDgm"
      },
      "source": [
        "<a name=\"api\"></a>\n",
        "## The API\n",
        "\n",
        "Now that we have fathomnet-py installed, let's see what it can do!\n",
        "\n",
        "This section will show some of the common calls to pull down FathomNet data, and then we'll render some visualizations of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkLXdOx-F6Gm"
      },
      "source": [
        "### Overview\n",
        "\n",
        "The two main parts of fathomnet-py are the **modules** and the **data models**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlGSfHJkKKsy"
      },
      "source": [
        "#### Modules\n",
        "\n",
        "fathomnet-py offers a variety of modules that encapsulate their relevant API operations. In brief:\n",
        "\n",
        "- `boundingboxes` --- find & manage bounding boxes\n",
        "- `darwincore` --- list owner institutions\n",
        "- `images` --- find & manage images\n",
        "- `geoimages` --- query for geo-images (geographic info only of images)\n",
        "- `imagesetuploads` --- find & manage image set uploads\n",
        "- `regions` --- list marine regions\n",
        "- `stats` --- compute summary statistics\n",
        "- `tags` --- find & manage custom image tags\n",
        "- `taxa` --- get taxonomic information via a taxa provider\n",
        "- `users` --- manage user accounts & list contributors\n",
        "- `firebase` & `xapikey` -- authenticate for write-level operations\n",
        "\n",
        "*Note: We will repeatedly import some of these modules in the notebook to highlight what's being used in each step. In your code, you only need to import a module once.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSVuXOeAFYcO"
      },
      "source": [
        "Each operation (API call) is represented as a function in its given module. For example, to get an image by its universally-unique identifier (UUID), we can import the `fathomnet.api.images` module and call the `find_by_uuid` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqyGxrO-LSxP"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import images\n",
        "\n",
        "example_image = images.find_by_uuid('79958ac5-832a-488c-9b48-cce7db346497')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCkHPRDcFzJn"
      },
      "source": [
        "#### Data models\n",
        "\n",
        "To facilitate parsing and saving FathomNet data, native Python data models (dataclasses) are provided in the `fathomnet.models` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGwVcnkqGsXE"
      },
      "source": [
        "For example, we can see that the returned image from the `find_by_uuid` call above is of type `AImageDTO`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nneeYpzEG0EV"
      },
      "outputs": [],
      "source": [
        "type(example_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1pBTzGQIQVL"
      },
      "source": [
        "These native data representations make it easier to write Python programs around FathomNet data. We'll print out some of the fields here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IzMAU88IpGy"
      },
      "outputs": [],
      "source": [
        "print('Image URL:', example_image.url)\n",
        "\n",
        "print('Captured at latitude/longitude', example_image.latitude, example_image.longitude)\n",
        "\n",
        "print('There are', len(example_image.boundingBoxes), 'bounding boxes:')\n",
        "for box in example_image.boundingBoxes:\n",
        "  print('-', box.concept, 'has area', box.width * box.height)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nLFG4fNHfhL"
      },
      "source": [
        "We can convert (serialize/deserialize) any of the FathomNet models to/from JSON or Python dictionaries. Let's print out the contents of that example image as JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvT2OyrcHy_K"
      },
      "outputs": [],
      "source": [
        "print(example_image.to_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em4qC6YUKade"
      },
      "source": [
        "### Bar chart of concepts with the most bounding boxes\n",
        "\n",
        "Here we will use a `boundingboxes` operation, called `count_total_by_concept`, to get a quick count of the total number of bounding boxes for every concept in FathomNet. To visualize, we'll make a bar chart of the top `N`.\n",
        "\n",
        "âš™ Try changing the value of `N` to show more concepts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLcJ0cHaPVoC"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import boundingboxes\n",
        "\n",
        "# Make a bar chart of the top N concepts by bounding boxes\n",
        "N = 5\n",
        "\n",
        "# Get the number of bounding boxes for all concepts\n",
        "concept_counts = boundingboxes.count_total_by_concept()\n",
        "\n",
        "# Print out the total number of concepts\n",
        "print('FathomNet has', len(concept_counts), 'localized concepts! Here are the top', N, 'by bounding box count.')\n",
        "\n",
        "# Sort by number of bounding boxes\n",
        "concept_counts.sort(key=lambda cc: cc.count, reverse=True)\n",
        "\n",
        "# Get the top N concepts and their counts\n",
        "concepts, counts = zip(*((cc.concept, cc.count) for cc in concept_counts[:N]))\n",
        "\n",
        "# Make a bar chart\n",
        "fig = px.bar(\n",
        "    x=concepts, y=counts, \n",
        "    labels={'x': 'Concept', 'y': 'Bounding box count'}, \n",
        "    title=f'Top {N} concepts', \n",
        "    text_auto=True\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL-_pukeZcXB"
      },
      "source": [
        "### Listing images for a concept\n",
        "\n",
        "Let's say we want to list all of the available images in FathomNet for a given concept. Here, we'll\n",
        "1. List all the available concepts (again, using the `boundingboxes` module)\n",
        "2. Pick one\n",
        "3. Get a list of images for that concept using the `images` module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS585C0JZnqD"
      },
      "source": [
        "First, let's list all the available concepts in a choosable box.\n",
        "\n",
        "We'll call the `find_concepts` function and put the results in a combo box.\n",
        "\n",
        "âš™ **Pick a concept after running this cell!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOc40XMNaItV"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import boundingboxes\n",
        "\n",
        "# Get a list of all concepts that have at least 1 bounding box\n",
        "all_concepts = boundingboxes.find_concepts()\n",
        "\n",
        "# Pick one!\n",
        "concept_combo = widgets.Combobox(\n",
        "    options=all_concepts,\n",
        "    description='Concept:',\n",
        "    placeholder='Double-click or type here',\n",
        "    ensure_option=True,\n",
        "    disabled=False\n",
        ")\n",
        "concept_combo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJJzxjPYOZQY"
      },
      "source": [
        "With our concept selected (if you didn't put anything, it will default to *Chionoecetes tanneri*), we can call the `images` module `find_by_concept` function to get back a list of all images containing a bounding box for that concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NJoDLJ6ueCx"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import images\n",
        "\n",
        "# Get the selected concept\n",
        "selected_concept = concept_combo.value\n",
        "if not selected_concept:\n",
        "  selected_concept = 'Chionoecetes tanneri'\n",
        "  print('No concept selected. Using the default:', selected_concept)\n",
        "\n",
        "# List the images FathomNet for that concept\n",
        "concept_images = images.find_by_concept(selected_concept)\n",
        "\n",
        "# Print the total number\n",
        "print('Found', len(concept_images), 'images of', selected_concept)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BzezsyGPJ0w"
      },
      "source": [
        "This next cell will pick a random image, fetch it by its URL, and display it. \n",
        "\n",
        "âš™ If you want a different image, just re-run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNmguhajsqzV"
      },
      "outputs": [],
      "source": [
        "# Pick a random image\n",
        "random_image = concept_images[random.randrange(len(concept_images))]\n",
        "\n",
        "# Fetch and show the image\n",
        "image_data = requests.get(random_image.url).content\n",
        "pil_image = Image.open(BytesIO(image_data))\n",
        "display(pil_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTD8_IRhjIjp"
      },
      "outputs": [],
      "source": [
        "# Concept -> color mapping for bounding boxes\n",
        "def color_for_concept(concept: str):\n",
        "  hash = sum(map(ord, concept)) << 5\n",
        "  return f'hsl({hash % 360}, 100%, 85%)'\n",
        "\n",
        "# Draw the bounding boxes and labels on the image\n",
        "draw_image = ImageDraw.Draw(pil_image)\n",
        "font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', size=18)\n",
        "for box in random_image.boundingBoxes:\n",
        "  color = color_for_concept(box.concept)\n",
        "  draw_image.rectangle((box.x, box.y, box.x + box.width, box.y + box.height), width=3, outline=color)\n",
        "  draw_image.text((box.x, box.y + box.height), box.concept, fill=color, font=font)\n",
        "\n",
        "# Show the image with overlay\n",
        "display(pil_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKZhbKeXWgHd"
      },
      "source": [
        "### Depth histogram\n",
        "\n",
        "Let's generate a depth histogram; we'll extract the `depthMeters` field from each image (where present) and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUhMWpTzv-es"
      },
      "outputs": [],
      "source": [
        "# Extract the depth (in meters) from each image\n",
        "depths = [\n",
        "    image.depthMeters\n",
        "    for image in concept_images \n",
        "    if image.depthMeters is not None\n",
        "]\n",
        "\n",
        "# Make a horizontal histogram\n",
        "fig = px.histogram(y=depths, title=f'{selected_concept} images by depth', labels={'y': 'depth (m)'})\n",
        "fig['layout']['yaxis']['autorange'] = 'reversed'\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrtWydOLWXM0"
      },
      "source": [
        "### Geographic heatmap\n",
        "\n",
        "We can use the `latitude` and `longitude` fields to georeference each image. Here, we're generating a heatmap of the images overlaid on the Esri ocean basemap.\n",
        "\n",
        "âš™ Zoom and pan around -- although the map is centered on the Monterey Bay, see if you can find where other \"hotspots\" are for your concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeR_4Ad-wT-t"
      },
      "outputs": [],
      "source": [
        "# Extract the latitude/longitude from each image\n",
        "locations = [\n",
        "    (image.latitude, image.longitude)\n",
        "    for image in concept_images\n",
        "    if image.latitude is not None and image.longitude is not None\n",
        "]\n",
        "\n",
        "# Create a map from the Esri Ocean basemap\n",
        "center = (36.807, -121.988)  # Monterey Bay\n",
        "map = ipyleaflet.Map(\n",
        "    basemap=ipyleaflet.basemaps.Esri.OceanBasemap, \n",
        "    center=center, \n",
        "    zoom=10\n",
        ")\n",
        "map.layout.height = \"800px\"\n",
        "\n",
        "# Overlay the image locations as a heatmap\n",
        "heatmap = ipyleaflet.Heatmap(\n",
        "    locations=locations,\n",
        "    radius=20,\n",
        "    min_opacity=0.5\n",
        ")\n",
        "map.add_layer(heatmap)\n",
        "\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4oqvJXTWq__"
      },
      "source": [
        "<a name=\"usecase\"></a>\n",
        "## Example use case\n",
        "\n",
        "**Object detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjUISR3bsIZA"
      },
      "source": [
        "### Get training data\n",
        "\n",
        "Let's take a look at how we can leverage the Python API to download some images and bounding boxes from FathomNet. We'll use these images to train a model later-on, so make sure you've completed this section before proceeding in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYXNIUQEnazh"
      },
      "source": [
        "#### Query for what we want\n",
        "\n",
        "As we saw before, we can use the `fathomnet.api.images` module to search for images by concept. But what if we need to fine-tune our query?\n",
        "\n",
        "Let's make a laundry list...\n",
        "- 50 images of *Gersemia juliepackardae*\n",
        "- 50 images of anything > 1000 m in depth\n",
        "\n",
        "We can find this data by specifying a *constraint object* in fathomnet-py, then calling a generalized image querying function. To do this, we need to grab `GeoImageConstraints` from the `fathomnet.models` module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6ISJr-Jo5La"
      },
      "outputs": [],
      "source": [
        "from fathomnet.models import GeoImageConstraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJR4A47opIpo"
      },
      "source": [
        "Now, we can make a set of constraints for each bullet point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVKnacl7pP9X"
      },
      "outputs": [],
      "source": [
        "gersemia_constraints = GeoImageConstraints(concept='Gersemia juliepackardae', limit=50)\n",
        "min_depth_constraints = GeoImageConstraints(minDepth=1000.0, limit=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LcXgSkkpj1P"
      },
      "source": [
        "To query for image data according to these constraints, we'll call the `fathomnet.api.images.find` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsQCu2_rpsIo"
      },
      "outputs": [],
      "source": [
        "gersemia_images = images.find(gersemia_constraints)\n",
        "print(f'Gersemia juliepackardae images: {len(gersemia_images)}')\n",
        "\n",
        "min_depth_images = images.find(min_depth_constraints)\n",
        "print(f'>1000m images: {len(min_depth_images)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBARsmUWsI9B"
      },
      "source": [
        "The bounding boxes are included in the image data. Let's take a look at how many boxes we have per concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwH6-OI9sH8I"
      },
      "outputs": [],
      "source": [
        "# Collect unique images (make sure we don't have overlap)\n",
        "all_images = []\n",
        "all_images.extend(gersemia_images)\n",
        "all_images.extend(im for im in min_depth_images if im not in all_images)\n",
        "\n",
        "# Count how many boxes we have per concept\n",
        "concept_counts = {}\n",
        "for image in all_images:\n",
        "  for box in image.boundingBoxes:\n",
        "    if box.concept not in concept_counts:\n",
        "      concept_counts[box.concept] = 0\n",
        "    concept_counts[box.concept] += 1\n",
        "\n",
        "for concept, count in sorted(concept_counts.items()):\n",
        "  print(f'{count:4} {concept}')\n",
        "print('-'*20)\n",
        "print(f'{sum(concept_counts.values()):4} total')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et78f9t_yxYl"
      },
      "source": [
        "Sweet! Now we know what we're working with. But, in order to get this data ready for training, we still need to do two things:\n",
        "1. **Download** the images themselves\n",
        "2. **Format** the bounding boxes into something the model can understand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvSMZgIL0NAa"
      },
      "source": [
        "#### Download the images\n",
        "\n",
        "No magic here, we just need to download the images (via HTTP) to somewhere the notebook can find them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBg0I7b80f2V"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlretrieve\n",
        "from urllib.parse import urlparse\n",
        "from pathlib import Path\n",
        "from progressbar import progressbar\n",
        "\n",
        "# Create a directory for the images\n",
        "data_dir = Path('/content/gersemia_voc')\n",
        "image_dir = data_dir / 'JPEGImages'\n",
        "image_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Download each image, saving each new file path to a list\n",
        "image_paths = []\n",
        "for image in progressbar(all_images, redirect_stdout=True):\n",
        "  parsed_url = urlparse(image.url)\n",
        "  url_path = Path(parsed_url.path)\n",
        "  image_path = image_dir / Path(image.uuid).with_suffix(url_path.suffix)\n",
        "  urlretrieve(image.url, image_path)\n",
        "  image_paths.append(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6sNnz_t23Y9"
      },
      "source": [
        "#### Format the bounding boxes\n",
        "\n",
        "We need to get the bounding boxes in a format the model can understand. Image data (`AImageDTO`) objects offer a convenience function to generate Pascal VOC annotations from their internal data. We can leverage this to quickly generate XML annotations of the form:\n",
        "\n",
        "```xml\n",
        "<annotation>\n",
        "  <folder>images</folder>\n",
        "  <filename>{image filename}</filename>\n",
        "  <path>/content/drive/MyDrive/fathomnet-workshop-tests/images/{image filename}</path>\n",
        "  <source>\n",
        "    <database>FathomNet</database>\n",
        "  </source>\n",
        "  <size>\n",
        "    <width>{image width}</width>\n",
        "    <height>{image height}</height>\n",
        "    <depth>3</depth>\n",
        "  </size>\n",
        "  <segmented>0</segmented>\n",
        "  <object>\n",
        "    <name>{concept}[ ({altConcept})]</name>\n",
        "    <pose>Unspecified</pose>\n",
        "    <truncated>0</truncated>\n",
        "    <difficult>0</difficult>\n",
        "    <occluded>0</occluded>\n",
        "    <bndbox>\n",
        "      <xmin>{x}</xmin>\n",
        "      <xmax>{x + width}</xmax>\n",
        "      <ymin>{y}</ymin>\n",
        "      <ymax>{y + height}</ymax>\n",
        "    </bndbox>\n",
        "  </object>\n",
        "  ...\n",
        "</annotation>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJgnWhcG3b-T"
      },
      "outputs": [],
      "source": [
        "xml_dir = data_dir / 'Annotations'\n",
        "xml_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "for image, image_path in zip(all_images, image_paths):\n",
        "  xml_path = xml_dir / image_path.with_suffix('.xml').name\n",
        "\n",
        "  pascal_voc = image.to_pascal_voc(path=str(image_path), pretty_print=True)\n",
        "  xml_path.write_text(pascal_voc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrTNntfuGRLQ"
      },
      "source": [
        "### Inference with a pre-trained model\n",
        "\n",
        "There are loads of software tools out there to train and run deep learning models. For this tutorial we will use [Detectron2](https://github.com/facebookresearch/detectron2), Facebook Research's machine learning library. This is just one of many options. \n",
        "\n",
        "To start with, we'll need grab a bunch of software directly from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA_RszjyCHJ1"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEGu4vYeaYtX"
      },
      "source": [
        "Double check that the fathomnet tools are installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJlD72X3F6Tp"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U fathomnet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIoypIxuR1W1"
      },
      "source": [
        "Then import all the needed libraries into the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kAT9rIzD-nN"
      },
      "outputs": [],
      "source": [
        "import detectron2                               # core deep learning library\n",
        "import torchvision                              # library of datasets, models, and image transforms\n",
        "import pickle                                   # serialization library data\n",
        "import json                                     # data storage standard\n",
        "import matplotlib.pyplot as plt                 # plotting utilities\n",
        "import torch                                    # tensor library for manipulating large models and data\n",
        "import requests                                 # Manages HTTP requests\n",
        "import random                                   # random number generator\n",
        "import numpy as np                              # array manipulations\n",
        "from skimage import io                          # use skimage to load individual images from a url\n",
        "from fathomnet.api import images, boundingboxes # use fathomnet to find images\n",
        "\n",
        "# import specific functions from detectron for shorter syntax when calling\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data import Metadata\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.modeling import build_model\n",
        "import detectron2.data.transforms as T\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "\n",
        "# import specific libraries from pyplot and PIL for easy plotting\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image\n",
        "\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhJe6TvnsYc9"
      },
      "source": [
        "#### Download a model from the FathomNet model zoo\n",
        "A big feature of FathomNet is the *ModelZoo*, a repository for users to share their models with the community. For the moment, [we are advising users](https://medium.com/fathomnet/how-to-upload-your-ml-model-to-fathomnet-68b933dd55bd) to upload their models on Zenodo to generate a DOI and then share them on our GitHub page.  We have provided a number of our models as a starting point. \n",
        "\n",
        "For this section of the workshop, we'll download the [MBARI Benthic Supercategory Detector](https://zenodo.org/record/5571043). This Retinanet model was fine tuned with FathomNet data from a version originally trained on COCO images. To train this system, we grouped many of our fine grained classes together into 20 'supercategories' that hopefully encode some generally morphological informatoin about the group. All the training data was drawn from MBARI imagery collected in Monterey Bay. \n",
        "\n",
        "We will need to make sure that Colab can talk to your personal Google Drive in order to download training images, pretrained model weights, etc. This command will open a pop-up window asking you which account link with and for permission to access files. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STg1ngP7Fyzs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "try:\n",
        "  drive.mount('/content/drive')\n",
        "except ValueError:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v4WaQgVIGAi"
      },
      "source": [
        "We will run `wget` to actually do the download. This command will let Colab download resources from a URL. We will start by getting the weights from the \n",
        "\n",
        "The syntax for `wget` is: \n",
        "\n",
        "```\n",
        "wget <url-to-download> --directory-prefix=<content/drive/MyDrive/my-workshop-directory>\n",
        "```\n",
        "\n",
        "The first set of brackets points to the desired URL, the second is the location of the output directory on your drive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anjZTfVEg5nA"
      },
      "source": [
        "First, let's download the model weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg41P_T_IFQx"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://zenodo.org/record/5571043/files/model_final.pth --directory-prefix=/content/drive/MyDrive/fathomnet-workshop-tests/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZB9LkTkOjiX"
      },
      "source": [
        "You should now be able to go into your Google Drive account and see the weights downloaded into your workshop directory.\n",
        "\n",
        "Now we'll grab the model file that declares the structure of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lVKx77wO944"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://zenodo.org/record/5571043/files/fathomnet_config_v2_1280.yaml --directory-prefix=/content/drive/MyDrive/fathomnet-workshop-tests/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRsLNBqlsdEy"
      },
      "source": [
        "#### Run inference\n",
        "We can actually run images through our network now that we have the model architecture and the weights from training. Before we run anything we will need to load the model into memory and set several parameters that will dictate what we see in the output. \n",
        "\n",
        "First set the paths so the `detectron2` toolbox will know where to look for your files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnS-jK4MWu2m"
      },
      "outputs": [],
      "source": [
        "CONFIG_FILE = \"/content/drive/MyDrive/fathomnet-workshop-tests/fathomnet_config_v2_1280.yaml\"   # training configuration file\n",
        "WEIGHT_FILE = \"/content/drive/MyDrive/fathomnet-workshop-tests/model_final.pth\"                 # fathomnet model weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4jJAAAyb36c"
      },
      "source": [
        "Now set Non-Maximal Suppresion (NMS) and Score thresholds. These parameters dictate which of the proposed regions the algorithm displays. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02DDZCQ-dmyz"
      },
      "outputs": [],
      "source": [
        "NMS_THRESH = 0.45   # Set an NMS threshold to filter all the boxes proposed by the model\n",
        "SCORE_THRESH = 0.3  # Set the model score threshold to suppress low confidence annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdrEGCt3d5pI"
      },
      "source": [
        "You have to explicitly tell the model what the names of the classes are. The system outputs a number, not a label. You can think of this as a look-up table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i-EmxtXeEIB"
      },
      "outputs": [],
      "source": [
        "fathomnet_metadata = Metadata(\n",
        "    name='fathomnet_val',\n",
        "    thing_classes=[\n",
        "         'Anemone',\n",
        "         'Fish',\n",
        "         'Eel',\n",
        "         'Gastropod',\n",
        "         'Sea star',\n",
        "         'Feather star',\n",
        "         'Sea cucumber',\n",
        "         'Urchin',\n",
        "         'Glass sponge',\n",
        "         'Sea fan',\n",
        "         'Soft coral',\n",
        "         'Sea pen',\n",
        "         'Stony coral',\n",
        "         'Ray',\n",
        "         'Crab',\n",
        "         'Shrimp',\n",
        "         'Squat lobster',\n",
        "         'Flatfish',\n",
        "         'Sea spider',\n",
        "         'Worm'\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KA9vCCweOZH"
      },
      "source": [
        "With all the parameters and file paths set up, you can now point Detectron to the configurations using the `get_cfg()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC50iab9eIne"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
        "cfg.merge_from_file(CONFIG_FILE)\n",
        "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = SCORE_THRESH\n",
        "cfg.MODEL.WEIGHTS = WEIGHT_FILE "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akq8aB8ieik3"
      },
      "source": [
        "Load in all the model weights and set the thresholds. This actually instantiates the model in your workspace. The `model` object is what will ingest the images and return outputs for us to look at. \n",
        "\n",
        "âš  *If this cell returns a* `RuntimeError: No CUDA GPUs are available` *you will need to update your settings. Click the Runtime dropdown menu, select \"Change runtime type\" and select GPU in the \"Hardware accelarator\" box. You will then need to rerun the detectron2 install via pip.*  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrtudCKeefcM"
      },
      "outputs": [],
      "source": [
        "model = build_model(cfg)                      # returns a torch.nn.Module\n",
        "checkpointer = DetectionCheckpointer(model)\n",
        "checkpointer.load(cfg.MODEL.WEIGHTS)          # This sets the weights to the pre-trained values dowloaded from Zenodo\n",
        "model.eval()                                  # Tell detectron that this model will only run inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HELDrDbTInnO"
      },
      "source": [
        "Before putting images through the network, you need to define some preprocessing steps. At training time, you might set up a series of random affine transformations to help guard against overfitting. Since this network is already trained, we just need to resize time images to a standard dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTIOGKlQI7nX"
      },
      "outputs": [],
      "source": [
        "aug = T.ResizeShortestEdge(\n",
        "    short_edge_length=[cfg.INPUT.MIN_SIZE_TEST], \n",
        "    max_size=cfg.INPUT.MAX_SIZE_TEST, \n",
        "    sample_style=\"choice\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbOymMvZJLH2"
      },
      "source": [
        "Finally, we need to set up an extra NMS layer since by default `detectron2` models only do intra-class comparisions between bounding boxes. We need to do another NMS run between classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAomTUA2O2s2"
      },
      "outputs": [],
      "source": [
        "post_process_nms = torchvision.ops.nms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnL6nUpqYyzu"
      },
      "source": [
        "We'll need to grab a random (or not so random) image to run through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRm3aVQwGbab"
      },
      "outputs": [],
      "source": [
        "# get a list of all concepts\n",
        "all_concepts = boundingboxes.find_concepts()\n",
        "\n",
        "# pick one at random (or set one yourself)\n",
        "#concept = 'Chionoecetes tanneri'\n",
        "concept = all_concepts[random.randrange(len(all_concepts))]\n",
        "\n",
        "# Pull the images from FathomNet\n",
        "concept_images = images.find_by_concept(concept)\n",
        "\n",
        "print(f'{len(concept_images)} images of {concept}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxMyTgddZ_Ro"
      },
      "source": [
        "Finally, you have everything loaded up to run the image through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piXtsgAbAwlI"
      },
      "outputs": [],
      "source": [
        "# Pick a random image\n",
        "image = concept_images[random.randrange(len(concept_images))]\n",
        "\n",
        "# Fetch the image\n",
        "im = io.imread(image.url)\n",
        "\n",
        "im_height,im_width,_ = im.shape  # grab the image dimensions\n",
        "\n",
        "# Use detectron's visualization tool to plot the bounding boxes\n",
        "v_inf = Visualizer(\n",
        "    im,\n",
        "    metadata=fathomnet_metadata, \n",
        "    scale=1.0, \n",
        "    instance_mode=ColorMode.IMAGE\n",
        ")\n",
        "\n",
        "# Transform the image in the desired input shape\n",
        "im_transformed = aug.get_transform(im).apply_image(im)\n",
        "\n",
        "# Actually crank it through the model\n",
        "with torch.no_grad():\n",
        "    im_tensor = torch.as_tensor(im_transformed.astype(\"float32\").transpose(2, 0, 1))\n",
        "    model_outputs = model([{\"image\" : im_tensor, \"height\" : im_height, \"width\" : im_width}])[0]\n",
        "\n",
        "# Run the second stage NMS to ensure limited interclass overlap\n",
        "model_outputs[\"instances\"] = model_outputs[\"instances\"][post_process_nms(model_outputs[\"instances\"].pred_boxes.tensor, model_outputs[\"instances\"].scores, NMS_THRESH).to(\"cpu\").tolist()]\n",
        "\n",
        "# Use the visualization tool to plot the bounding boxes on top of the image\n",
        "out_inf_raw = v_inf.draw_instance_predictions(model_outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "# Then display the output\n",
        "fig, ax = plt.subplots(figsize=(24,16))\n",
        "ax.imshow(out_inf_raw.get_image())\n",
        "ax.set_axis_off()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh_fpnNBsMo-"
      },
      "source": [
        "### Train a model\n",
        "\n",
        "Training a model takes time, computational resources, and interative evaluation. To illustrate the beginnings of this process, we will fine tune a (relatively) small object detector to find the soft coral *Gersemia juliepackardae* in benthic images.\n",
        "\n",
        "We have already loaded the images in with a VOC file format. Detectron2 has a pre-made dataloader to finesse the dataset into the model training cycle. But first we need to specify lists of training and validation data. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "all_imgs = glob.glob(os.path.join(data_dir / 'JPEGImages'))\n",
        "\n",
        "print(data_dir / 'images')"
      ],
      "metadata": {
        "id": "B-u97DvWAwFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data.datasets import register_pascal_voc\n",
        "\n",
        "\n",
        "\n",
        "register_pascal_voc(\"gersemia_train\", data_dir, \"train\", class_names='Gersemia juliepackardae')"
      ],
      "metadata": {
        "id": "Tmqx5ch7-iR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GfpxmIywAH1t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pkLXdOx-F6Gm"
      ],
      "name": "FathomNet Python API Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}