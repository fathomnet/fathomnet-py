{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifmDW0dirq9D"
      },
      "source": [
        "# FathomNet Python API Tutorial\n",
        "*So you want to use FathomNet data...*\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/fathomnet/fathomnet-logo/main/FathomNet_white_CenterText_400px.png\" alt=\"FathomNet logo\" width=\"200\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNYBxwg3UsZe"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "> `fathomnet-py` is a client-side API to help scientists, researchers, and developers interact with FathomNet data.\n",
        "\n",
        "[![tests](https://github.com/fathomnet/fathomnet-py/actions/workflows/tests.yml/badge.svg)](https://github.com/fathomnet/fathomnet-py/actions/workflows/tests.yml)\n",
        "[![Documentation Status](https://readthedocs.org/projects/fathomnet-py/badge/?version=latest)](https://fathomnet-py.readthedocs.io/en/latest/?badge=latest)\n",
        "\n",
        "The [fathomnet-py](https://github.com/fathomnet/fathomnet-py) API offers native Python interaction with the FathomNet REST API, abstracting away the underlying HTTP requests.\n",
        "This notebook is designed to walk you through some of the core functionality of the API and to illustrate a common use case: *training an object detector*. \n",
        "\n",
        "It's split into two parts:\n",
        "1. [**The API**](#api): API overview and data visualizations\n",
        "2. [**Example use case**](#usecase): training an object detector and running inference\n",
        "\n",
        "This notebook is by no means exhaustive; it serves to show some common \"recipes\" for pulling down and handling FathomNet data in Python. **Full documentation for fathomnet-py is available at [fathomnet-py.readthedocs.io](https://fathomnet-py.readthedocs.io).**\n",
        "\n",
        "[FathomNet GitHub](https://github.com/fathomnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM6WRbgWWkV1"
      },
      "source": [
        "### Installing `fathomnet-py`\n",
        "\n",
        "To install fathomnet-py, you will need to have Python 3.7 or greater installed first (as of the time of writing, this notebook ships with Python 3.9). Then, from the command-line:\n",
        "\n",
        "```bash\n",
        "pip install fathomnet\n",
        "```\n",
        "\n",
        "This notebook installs fathomnet-py in the [Setup](#setup) section next, along with some relevant packages for data manipulation and visualization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkYCmUX6r0su"
      },
      "source": [
        "<a name=\"setup\"></a>\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uNeHCauXyvC"
      },
      "source": [
        "First, we'll install a few packages via pip:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q--V_Xrr3js"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U fathomnet ipyleaflet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfilAbptX2LU"
      },
      "source": [
        "and import the auxiliary modules we need for part 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QpcYIXG-sofq"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets                      # Provides embedded widgets\n",
        "import ipyleaflet                                 # Provides map widgets\n",
        "import requests                                   # Manages HTTP requests\n",
        "import numpy as np                                # Facilitates array/matrix operations\n",
        "import plotly.express as px                       # Generates nice plots\n",
        "import random                                     # Generates pseudo-random numbers\n",
        "from PIL import Image, ImageFont, ImageDraw       # Facilitates image operations\n",
        "from io import BytesIO                            # Interfaces byte data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iazt9ae-sDgm"
      },
      "source": [
        "<a name=\"api\"></a>\n",
        "## The API\n",
        "\n",
        "Now that we have fathomnet-py installed, let's see what it can do!\n",
        "\n",
        "This section will show some of the common calls to pull down FathomNet data, and then we'll render some visualizations of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkLXdOx-F6Gm"
      },
      "source": [
        "### Overview\n",
        "\n",
        "The two main parts of fathomnet-py are the **modules** and the **data models**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlGSfHJkKKsy"
      },
      "source": [
        "#### Modules\n",
        "\n",
        "fathomnet-py offers a variety of modules that encapsulate their relevant API operations. In brief:\n",
        "\n",
        "- `boundingboxes` --- find & manage bounding boxes\n",
        "- `darwincore` --- list owner institutions\n",
        "- `images` --- find & manage images\n",
        "- `geoimages` --- query for geo-images (geographic info only of images)\n",
        "- `imagesetuploads` --- find & manage image set uploads\n",
        "- `regions` --- list marine regions\n",
        "- `stats` --- compute summary statistics\n",
        "- `tags` --- find & manage custom image tags\n",
        "- `taxa` --- get taxonomic information via a taxa provider\n",
        "- `users` --- manage user accounts & list contributors\n",
        "- `firebase` & `xapikey` -- authenticate for write-level operations\n",
        "\n",
        "*Note: We will repeatedly import some of these modules in the notebook to highlight what's being used in each step. In your code, you only need to import a module once.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSVuXOeAFYcO"
      },
      "source": [
        "Each operation (API call) is represented as a function in its given module. For example, to get an image by its universally-unique identifier (UUID), we can import the `fathomnet.api.images` module and call the `find_by_uuid` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqyGxrO-LSxP"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import images\n",
        "\n",
        "example_image = images.find_by_uuid('79958ac5-832a-488c-9b48-cce7db346497')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCkHPRDcFzJn"
      },
      "source": [
        "#### Data models\n",
        "\n",
        "To facilitate parsing and saving FathomNet data, native Python data models (dataclasses) are provided in the `fathomnet.models` module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGwVcnkqGsXE"
      },
      "source": [
        "For example, we can see that the returned image from the `find_by_uuid` call above is of type `AImageDTO`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nneeYpzEG0EV"
      },
      "outputs": [],
      "source": [
        "type(example_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1pBTzGQIQVL"
      },
      "source": [
        "These native data representations make it easier to write Python programs around FathomNet data. We'll print out some of the fields here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IzMAU88IpGy"
      },
      "outputs": [],
      "source": [
        "print('Image URL:', example_image.url)\n",
        "\n",
        "print('Captured at latitude/longitude', example_image.latitude, example_image.longitude)\n",
        "\n",
        "print('There are', len(example_image.boundingBoxes), 'bounding boxes:')\n",
        "for box in example_image.boundingBoxes:\n",
        "  print('-', box.concept, 'has area', box.width * box.height)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nLFG4fNHfhL"
      },
      "source": [
        "We can convert (serialize/deserialize) any of the FathomNet models to/from JSON or Python dictionaries. Let's print out the contents of that example image as JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvT2OyrcHy_K"
      },
      "outputs": [],
      "source": [
        "print(example_image.to_json(indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em4qC6YUKade"
      },
      "source": [
        "### Bar chart of concepts with the most bounding boxes\n",
        "\n",
        "Here we will use a `boundingboxes` operation, called `count_total_by_concept`, to get a quick count of the total number of bounding boxes for every concept in FathomNet. To visualize, we'll make a bar chart of the top `N`.\n",
        "\n",
        "⚙ Try changing the value of `N` on the right to show more concepts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLcJ0cHaPVoC"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import boundingboxes\n",
        "\n",
        "# Make a bar chart of the top N concepts by bounding boxes\n",
        "N = 11 #@param {type:\"slider\", min:5, max:20, step:1}\n",
        "\n",
        "# Get the number of bounding boxes for all concepts\n",
        "concept_counts = boundingboxes.count_total_by_concept()\n",
        "\n",
        "# Sort by number of bounding boxes\n",
        "concept_counts.sort(key=lambda cc: cc.count, reverse=True)\n",
        "\n",
        "# Get the top N concepts and their counts\n",
        "concepts, counts = zip(*((cc.concept, cc.count) for cc in concept_counts[:N]))\n",
        "\n",
        "# Make a bar chart\n",
        "fig = px.bar(\n",
        "  x=concepts, y=counts, \n",
        "  labels={'x': 'Concept', 'y': 'Bounding box count'}, \n",
        "  title=f'Top {N} concepts', \n",
        "  text_auto=True\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL-_pukeZcXB"
      },
      "source": [
        "### Listing images for a concept\n",
        "\n",
        "Let's say we want to list all of the available images in FathomNet for a given concept. Here, we'll\n",
        "1. List all the available concepts (again, using the `boundingboxes` module)\n",
        "2. Pick one\n",
        "3. Get a list of images for that concept using the `images` module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS585C0JZnqD"
      },
      "source": [
        "First, let's list all the available concepts in a choosable box.\n",
        "\n",
        "We'll call the `find_concepts` function and put the results in a combo box.\n",
        "\n",
        "⚙ **Pick a concept after running this cell!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOc40XMNaItV"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import boundingboxes\n",
        "\n",
        "# Get a list of all concepts that have at least 1 bounding box\n",
        "all_concepts = boundingboxes.find_concepts()\n",
        "\n",
        "# Print how many there are\n",
        "print('FathomNet has', len(concept_counts), 'localized concepts!')\n",
        "\n",
        "# Pick one!\n",
        "concept_combo = widgets.Combobox(\n",
        "  options=all_concepts,\n",
        "  description='Pick one:',\n",
        "  placeholder='Double-click or type here',\n",
        "  ensure_option=True,\n",
        "  disabled=False\n",
        ")\n",
        "concept_combo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJJzxjPYOZQY"
      },
      "source": [
        "With our concept selected (if you didn't put anything, it will default to *Chionoecetes tanneri*), we can call the `images` module `find_by_concept` function to get back a list of all images containing a bounding box for that concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NJoDLJ6ueCx"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import images\n",
        "\n",
        "# Get the selected concept\n",
        "selected_concept = concept_combo.value or 'Chionoecetes tanneri'\n",
        "\n",
        "# List the images FathomNet for that concept\n",
        "concept_images = images.find_by_concept(selected_concept)\n",
        "\n",
        "# Print the total number\n",
        "print('Found', len(concept_images), 'images of', selected_concept)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BzezsyGPJ0w"
      },
      "source": [
        "This next cell will pick a random image, fetch it by its URL, and display it. \n",
        "\n",
        "⚙ If you want a different image, just re-run this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNmguhajsqzV"
      },
      "outputs": [],
      "source": [
        "# Pick a random image\n",
        "random_image = concept_images[random.randrange(len(concept_images))]\n",
        "\n",
        "# Fetch and show the image\n",
        "image_data = requests.get(random_image.url).content\n",
        "pil_image = Image.open(BytesIO(image_data))\n",
        "display(pil_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUOvebwiqHzM"
      },
      "source": [
        "Then, we'll loop over each bounding box listed and render it (drawing a box & label for it) on the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTD8_IRhjIjp"
      },
      "outputs": [],
      "source": [
        "# Concept -> color mapping for bounding boxes\n",
        "def color_for_concept(concept: str):\n",
        "  hash = sum(map(ord, concept)) << 5\n",
        "  return f'hsl({hash % 360}, 100%, 85%)'\n",
        "\n",
        "# Draw the bounding boxes and labels on the image\n",
        "draw_image = ImageDraw.Draw(pil_image)\n",
        "font = ImageFont.truetype('/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf', size=18)\n",
        "for box in random_image.boundingBoxes:\n",
        "  color = color_for_concept(box.concept)\n",
        "  draw_image.rectangle((box.x, box.y, box.x + box.width, box.y + box.height), width=3, outline=color)\n",
        "  draw_image.text((box.x, box.y + box.height), box.concept, fill=color, font=font)\n",
        "\n",
        "# Show the image with overlay\n",
        "display(pil_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKZhbKeXWgHd"
      },
      "source": [
        "### Depth histogram\n",
        "\n",
        "Let's generate a depth histogram; we'll extract the `depthMeters` field from each image (where present) and plot it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUhMWpTzv-es"
      },
      "outputs": [],
      "source": [
        "# Extract the depth (in meters) from each image\n",
        "depths = [\n",
        "  image.depthMeters\n",
        "  for image in concept_images \n",
        "  if image.depthMeters is not None\n",
        "]\n",
        "\n",
        "# Make a horizontal histogram\n",
        "fig = px.histogram(y=depths, title=f'{selected_concept} images by depth', labels={'y': 'depth (m)'})\n",
        "fig['layout']['yaxis']['autorange'] = 'reversed'\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrtWydOLWXM0"
      },
      "source": [
        "### Geographic heatmap\n",
        "\n",
        "We can use the `latitude` and `longitude` fields to georeference each image. Here, we're generating a heatmap of the images overlaid on the Esri ocean basemap.\n",
        "\n",
        "⚙ Zoom and pan around -- although the map is centered on the Monterey Bay, see if you can find where other \"hotspots\" are for your concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeR_4Ad-wT-t"
      },
      "outputs": [],
      "source": [
        "# Extract the latitude/longitude from each image\n",
        "locations = [\n",
        "  (image.latitude, image.longitude)\n",
        "  for image in concept_images\n",
        "  if image.latitude is not None and image.longitude is not None\n",
        "]\n",
        "\n",
        "# Create a map from the Esri Ocean basemap\n",
        "center = (36.807, -121.988)  # Monterey Bay\n",
        "map = ipyleaflet.Map(\n",
        "  basemap=ipyleaflet.basemaps.Esri.OceanBasemap, \n",
        "  center=center, \n",
        "  zoom=10\n",
        ")\n",
        "map.layout.height = \"800px\"\n",
        "\n",
        "# Overlay the image locations as a heatmap\n",
        "heatmap = ipyleaflet.Heatmap(\n",
        "  locations=locations,\n",
        "  radius=20,\n",
        "  min_opacity=0.5\n",
        ")\n",
        "map.add_layer(heatmap)\n",
        "\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4oqvJXTWq__"
      },
      "source": [
        "<a name=\"usecase\"></a>\n",
        "## Example use case: **Object detection**\n",
        "\n",
        "There are loads of software tools out there to train and run deep learning models. For this tutorial we will use [Detectron2](https://github.com/facebookresearch/detectron2), Facebook Research's machine learning library. This is just one of many options. \n",
        "\n",
        "To start with, we'll need grab a bunch of software directly from GitHub. You may see a red ERROR message in this cell -- you can disregard it. **This will take a couple minutes.** Grab a coffee! Or ask us some questions! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KA_RszjyCHJ1"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U fathomnet pyyaml==5.4.1 'git+https://github.com/facebookresearch/detectron2.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODhwpyhxU8DW"
      },
      "source": [
        "⚠ Detectron doesn't play nice with some installed package versions; you may see a mesasge asking you to restart the runtime. Press that button, or run this cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egXwSPqvVJDM"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import detectron2\n",
        "except:\n",
        "  print('Restarting runtime...')\n",
        "  exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIoypIxuR1W1"
      },
      "source": [
        "Then import all the needed libraries into the workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kAT9rIzD-nN"
      },
      "outputs": [],
      "source": [
        "import torchvision                              # Library of datasets, models, and image transforms\n",
        "import pickle                                   # Data serialization library\n",
        "import json                                     # Data storage (JavaScript Object Notation)\n",
        "import matplotlib.pyplot as plt                 # Plotting utilities\n",
        "import torch                                    # Tensor library for manipulating large models and data\n",
        "import requests                                 # Manages HTTP requests\n",
        "import random                                   # Random number generator\n",
        "import numpy as np                              # Array manipulations\n",
        "\n",
        "# Import key functions & modules from detectron2\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data import Metadata\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "import detectron2.data.transforms as T\n",
        "\n",
        "# Import from pyplot and PIL for easy plotting\n",
        "from matplotlib.pyplot import imshow\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjUISR3bsIZA"
      },
      "source": [
        "### Get training data\n",
        "\n",
        "Let's take a look at how we can leverage the Python API to download some images and bounding boxes from FathomNet. We'll use these images to train a model later-on, so make sure you've completed this section before proceeding in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYXNIUQEnazh"
      },
      "source": [
        "#### Query for what we want\n",
        "\n",
        "As we saw before, we can use the `fathomnet.api.images` module to search for images by concept. But what if we need to fine-tune our query?\n",
        "\n",
        "Let's say we want to train an object detector to detect *Gersemia juliepackardae* only. We don't want too much training data, so let's limit our query to just 100 images of *G. juliepackardae*.\n",
        "\n",
        "We can find this data by specifying a *constraint object* in fathomnet-py, then calling a generalized image querying function. To do this, we need to grab `GeoImageConstraints` from the `fathomnet.models` module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6ISJr-Jo5La"
      },
      "outputs": [],
      "source": [
        "from fathomnet.models import GeoImageConstraints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJR4A47opIpo"
      },
      "source": [
        "Now, we can make a set of constraints for each bullet point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVKnacl7pP9X"
      },
      "outputs": [],
      "source": [
        "gersemia_constraints = GeoImageConstraints(concept='Gersemia juliepackardae', limit=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LcXgSkkpj1P"
      },
      "source": [
        "To query for image data according to these constraints, we'll call the `fathomnet.api.images.find` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsQCu2_rpsIo"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import images\n",
        "\n",
        "gersemia_images = images.find(gersemia_constraints)\n",
        "print(f'Gersemia juliepackardae images: {len(gersemia_images)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et78f9t_yxYl"
      },
      "source": [
        "In order to get this data ready for training, we still need to do two things:\n",
        "1. **Download** the images themselves\n",
        "2. **Format** the bounding boxes into something the model can understand\n",
        "3. **Structure** the directory according to the [perscribed VOC format](https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#expected-dataset-structure-for-pascal-voc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvSMZgIL0NAa"
      },
      "source": [
        "#### Download the images\n",
        "\n",
        "No magic here, we just need to download the images (via HTTP) to somewhere the notebook can find them.\n",
        "\n",
        "*Note: there are more efficient ways to do this.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBg0I7b80f2V"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "from progressbar import progressbar\n",
        "from io import BytesIO\n",
        "\n",
        "# Create a directory for the images\n",
        "data_dir = Path('/content/gersemia_voc')\n",
        "image_dir = data_dir / 'JPEGImages'\n",
        "image_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Download each image, saving each new file path to a list\n",
        "image_paths = []\n",
        "for image in progressbar(gersemia_images):\n",
        "  # Format our image file name as the image UUID + .jpg\n",
        "  image_path = image_dir / f'{image.uuid}.jpg'\n",
        "  image_paths.append(image_path)\n",
        "  if image_path.exists():  # Skip re-downloading images\n",
        "    continue\n",
        "  \n",
        "  # Download the image\n",
        "  image_raw = requests.get(image.url, stream=True).raw\n",
        "  pil_image = Image.open(image_raw)\n",
        "  \n",
        "  # Convert to RGB (ensures consistent colorspace)\n",
        "  pil_image = pil_image.convert('RGB')\n",
        "\n",
        "  # Save the image\n",
        "  pil_image.save(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9BBAzV34vtX"
      },
      "source": [
        "While we're at it, let's write the `train.txt` file that Detectron will need later on. This is just a text file containing the list of file names we will use to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDtfeF-f4301"
      },
      "outputs": [],
      "source": [
        "imageset_main_dir = data_dir / 'ImageSets' / 'Main'\n",
        "imageset_main_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "train_file = imageset_main_dir / 'train.txt'\n",
        "with open(train_file, 'w') as f:\n",
        "  for image_path in image_paths:\n",
        "    f.write(f'{image_path.stem}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6sNnz_t23Y9"
      },
      "source": [
        "#### Format the bounding boxes\n",
        "\n",
        "We need to get the bounding boxes in a format the model can understand. Image data (`AImageDTO`) objects offer a convenience function to generate Pascal VOC annotations from their internal data. We can leverage this to quickly generate XML annotations of the form:\n",
        "\n",
        "```xml\n",
        "<annotation>\n",
        "  <folder>images</folder>\n",
        "  <filename>{image filename}</filename>\n",
        "  <path>/content/drive/MyDrive/fathomnet-workshop-tests/images/{image filename}</path>\n",
        "  <source>\n",
        "    <database>FathomNet</database>\n",
        "  </source>\n",
        "  <size>\n",
        "    <width>{image width}</width>\n",
        "    <height>{image height}</height>\n",
        "    <depth>3</depth>\n",
        "  </size>\n",
        "  <segmented>0</segmented>\n",
        "  <object>\n",
        "    <name>{concept}[ ({altConcept})]</name>\n",
        "    <pose>Unspecified</pose>\n",
        "    <truncated>0</truncated>\n",
        "    <difficult>0</difficult>\n",
        "    <occluded>0</occluded>\n",
        "    <bndbox>\n",
        "      <xmin>{x}</xmin>\n",
        "      <xmax>{x + width}</xmax>\n",
        "      <ymin>{y}</ymin>\n",
        "      <ymax>{y + height}</ymax>\n",
        "    </bndbox>\n",
        "  </object>\n",
        "  ...\n",
        "</annotation>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y98JabkwIRC8"
      },
      "source": [
        "We will likewise write up all the annotations in the preferred VOC directory structure to make sure Detectron finds everything where it expects it. \n",
        "\n",
        "At the same time, we'll filter out any bounding boxes besides *G. juliepackardae*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJgnWhcG3b-T"
      },
      "outputs": [],
      "source": [
        "xml_dir = data_dir / 'Annotations'\n",
        "xml_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "for image, image_path in zip(gersemia_images, image_paths):\n",
        "  xml_path = xml_dir / image_path.with_suffix('.xml').name\n",
        "  image.boundingBoxes = list(filter(  # filter only Gersemia juliepackardae\n",
        "      lambda box: box.concept == 'Gersemia juliepackardae', image.boundingBoxes\n",
        "  ))\n",
        "  pascal_voc = image.to_pascal_voc(path=str(image_path), pretty_print=True)\n",
        "  xml_path.write_text(pascal_voc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wh_fpnNBsMo-"
      },
      "source": [
        "### Train a model\n",
        "\n",
        "Training a model takes time, computational resources, and interative evaluation. To illustrate the beginnings of this process, we will fine tune a (relatively) small object detector to find the soft coral *Gersemia juliepackardae* in benthic images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBmiCLf54mhO"
      },
      "source": [
        "#### Prepare VOC data for training\n",
        "Detectron has a pre-made `dataloader` to finesse the dataset into the model training cycle. We have already set up the data with the [directory structure](https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#expected-dataset-structure-for-pascal-voc) that Detectron is expecting. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tmqx5ch7-iR8"
      },
      "outputs": [],
      "source": [
        "from detectron2.data.datasets import register_pascal_voc\n",
        "from detectron2.data import DatasetCatalog\n",
        "from datetime import datetime\n",
        "\n",
        "# Clear the catalog\n",
        "DatasetCatalog.clear()\n",
        "\n",
        "# Try to register our new dataset\n",
        "try:\n",
        "  register_pascal_voc('gersemia_train', data_dir, 'train', datetime.now().year, class_names=('Gersemia juliepackardae',))\n",
        "  print('Dataset registered.')\n",
        "except AssertionError as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJZjTjWnKWXe"
      },
      "source": [
        "Remember that VOC is just one [common dataset format](https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#) that many deep learning packages play nicely with. The fathomnet-py API currently supports writing annotations according to [VOC](https://detectron2.readthedocs.io/en/latest/tutorials/builtin_datasets.html#expected-dataset-structure-for-pascal-voc) and [COCO instance](https://cocodataset.org/#format-data) standards.  \n",
        "\n",
        "Next we'll read in the [metadata](https://detectron2.readthedocs.io/en/latest/modules/data.html?highlight=MetadataCatalog#detectron2.data.MetadataCatalog) associated with the dataset we defined earlier. Detectron infers this information automatically based on the VOC directory structure. The toolkit treat this information like global variables that are accessible by many functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfpxmIywAH1t"
      },
      "outputs": [],
      "source": [
        "from detectron2.data import MetadataCatalog\n",
        "\n",
        "# Read our new metadata from the catalog\n",
        "gersemia_metadata = MetadataCatalog.get('gersemia_train')\n",
        "\n",
        "gersemia_metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCsiFGnpN9uE"
      },
      "source": [
        "#### Configure training cycle\n",
        "\n",
        "Now we need to set up the configuration file by giving Detectron a bunch more parameters to tell it how to train the network. There are LOTS of [settings you can configure](https://detectron2.readthedocs.io/en/latest/modules/config.html) and it is worth spending some time looking through the documentation of whatever deep learning framework you use to decide what you need. \n",
        "\n",
        "We are going to *fine tune* a network instead of training from scratch. Fine tuning is a procedure where we use weights from a network that was trained for a different task, strip off some of the top layers, and train for a little longer with new data. For this example, we are going to use a relatively small, quick to train version of [RetinaNet](https://openaccess.thecvf.com/content_iccv_2017/html/Lin_Focal_Loss_for_ICCV_2017_paper.html) trained on COCO that the [Detectron developer's provide](https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml) as a base. There are lots of other base models to experiment with provided in [their Model Zoo](https://github.com/facebookresearch/detectron2/blob/main/MODEL_ZOO.md). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBIz33XozI93"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "import gc\n",
        "import torch.cuda\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "BACKBONE = 'COCO-Detection/retinanet_R_50_FPN_3x.yaml'        # The backbone we'll use\n",
        "\n",
        "# Define the training configuration\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(BACKBONE))      # Retrieve the base network configuration\n",
        "cfg.DATASETS.TRAIN = ('gersemia_train',)                      # Tell it what to use for training\n",
        "cfg.DATASETS.TEST = ()                                        # Define test data (we are leaving this blank for speed)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2                                # The number of threads to start for moving images to the GPU\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(BACKBONE)    # Retrieve the weights for the desired base network\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8                                  # How many images to give the GPU at a time. Make this bigger if you have a more powerful card\n",
        "cfg.SOLVER.BASE_LR = 0.0025                                   # How much to move weights during backpropagation\n",
        "cfg.SOLVER.MAX_ITER = 50                                      # How many times to run the training images through the network\n",
        "cfg.SOLVER.STEPS = []                                         # When to change the learning rate. We are not making adjustements for this small training\n",
        "cfg.MODEL.RETINANET.NUM_CLASSES = 1                           # The number of output classes\n",
        "\n",
        "# Create the output directory\n",
        "Path(cfg.OUTPUT_DIR).mkdir(exist_ok=True, parents=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oNoNy4_4h8P"
      },
      "source": [
        "#### Run the trainer\n",
        "\n",
        "Detectron's [DefaultTrainer](https://detectron2.readthedocs.io/en/latest/modules/engine.html?highlight=defaulttrainer#detectron2.engine.defaults.DefaultTrainer) module makes a bunch of assumptions about how you want to execute your training. There is, of course, lots that you can adapt as needed when you want to train something a bit more complex. For our purposes it will work just fine. \n",
        "\n",
        "We set the number of training iterations to **50** for this demo. It will take a little over a minute to tune. Feel free to change that parameter and see what it does. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O771ZKTk4g9h"
      },
      "outputs": [],
      "source": [
        "# Spin up a trainer\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "\n",
        "# Train! This may take a while, depending on your instance type and config\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHqM0mDfTwiV"
      },
      "source": [
        "#### Predict\n",
        "\n",
        "Now that we've trained the model to find *Gersemia juliepackardae* we can test it using the Detectron prediction module. This tells the system that training is done and now it will just try to put bounding boxes on relevant objects. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRCxM9xI1fow"
      },
      "outputs": [],
      "source": [
        "from detectron2.engine import DefaultPredictor\n",
        "\n",
        "# Load the model weights into the configuration\n",
        "cfg.MODEL.WEIGHTS = str(Path(cfg.OUTPUT_DIR) / 'model_final.pth')\n",
        "\n",
        "# Set the confidence threshold (predictions with confidence < will be omitted)\n",
        "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n",
        "\n",
        "# Spin up a predictor with the provided config\n",
        "predictor = DefaultPredictor(cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVbAjH3ZUCLU"
      },
      "source": [
        "Now we can crank images through and see what comes out of the model! We are just drawing from the images we already downloaded, but feel free to try it with anything. \n",
        "\n",
        "⚙ Re-run this cell to run your model on a different set of random images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCwA2bKx91Jk"
      },
      "outputs": [],
      "source": [
        "from detectron2.utils.visualizer import ColorMode\n",
        "\n",
        "# Get 3 random images\n",
        "for image_path in random.sample(image_paths, 3):\n",
        "  # Open the image\n",
        "  pil_image = Image.open(image_path)\n",
        "  im = np.array(pil_image)\n",
        "\n",
        "  # Run it through the model\n",
        "  outputs = predictor(im)\n",
        "\n",
        "  # Render the predictions on the image\n",
        "  v = Visualizer(\n",
        "    im,\n",
        "    metadata=gersemia_metadata, \n",
        "    scale=0.5,\n",
        "  )\n",
        "  out = v.draw_instance_predictions(outputs['instances'].to('cpu'))\n",
        "  out_pil = Image.fromarray(out.get_image())\n",
        "\n",
        "  # Show it\n",
        "  display(out_pil)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrTNntfuGRLQ"
      },
      "source": [
        "### Inference with a pre-trained model\n",
        "\n",
        "A big feature of FathomNet is the *ModelZoo*, a repository for users to share their models with the community. For the moment, [we are advising users](https://medium.com/fathomnet/how-to-upload-your-ml-model-to-fathomnet-68b933dd55bd) to upload their models on Zenodo to generate a DOI and then share them on our GitHub page.  We have provided a number of our models as a starting point. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhJe6TvnsYc9"
      },
      "source": [
        "#### Download a model from the FathomNet model zoo\n",
        "For this section of the workshop, we'll download the [MBARI Benthic Supercategory Detector](https://zenodo.org/record/5571043). This Retinanet model was fine tuned with FathomNet data from a version originally trained on COCO images. To train this system, we grouped many of our fine grained classes together into 20 'supercategories' that hopefully encode some generally morphological informatoin about the group. All the training data was drawn from MBARI imagery collected in Monterey Bay. \n",
        "\n",
        "We will run `wget` to actually do the download. This command will let Colab download resources from a URL. We will start by getting the weights from the repository on Zenodo.\n",
        "\n",
        "First, let's download the model weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dg41P_T_IFQx"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://zenodo.org/record/5571043/files/model_final.pth "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZB9LkTkOjiX"
      },
      "source": [
        "Now we'll grab the model file that declares the structure of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lVKx77wO944"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://zenodo.org/record/5571043/files/fathomnet_config_v2_1280.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRsLNBqlsdEy"
      },
      "source": [
        "#### Run inference\n",
        "We can actually run images through our network now that we have the model architecture and the weights from training. Before we run anything we will need to load the model into memory and set several parameters that will dictate what we see in the output. \n",
        "\n",
        "First set the paths so the `detectron2` toolbox will know where to look for your files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnS-jK4MWu2m"
      },
      "outputs": [],
      "source": [
        "CONFIG_FILE = \"fathomnet_config_v2_1280.yaml\"   # training configuration file\n",
        "WEIGHT_FILE = \"model_final.pth\"                 # fathomnet model weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4jJAAAyb36c"
      },
      "source": [
        "Now set Non-Maximal Suppresion (NMS) and Score thresholds. These parameters dictate which of the proposed regions the algorithm displays. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02DDZCQ-dmyz"
      },
      "outputs": [],
      "source": [
        "NMS_THRESH = 0.45   # Set an NMS threshold to filter all the boxes proposed by the model\n",
        "SCORE_THRESH = 0.3  # Set the model score threshold to suppress low confidence annotations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdrEGCt3d5pI"
      },
      "source": [
        "You have to explicitly tell the model what the names of the classes are. The system outputs a number, not a label. You can think of this as a look-up table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i-EmxtXeEIB"
      },
      "outputs": [],
      "source": [
        "fathomnet_metadata = Metadata(\n",
        "  name='fathomnet_val',\n",
        "  thing_classes=[\n",
        "    'Anemone',\n",
        "    'Fish',\n",
        "    'Eel',\n",
        "    'Gastropod',\n",
        "    'Sea star',\n",
        "    'Feather star',\n",
        "    'Sea cucumber',\n",
        "    'Urchin',\n",
        "    'Glass sponge',\n",
        "    'Sea fan',\n",
        "    'Soft coral',\n",
        "    'Sea pen',\n",
        "    'Stony coral',\n",
        "    'Ray',\n",
        "    'Crab',\n",
        "    'Shrimp',\n",
        "    'Squat lobster',\n",
        "    'Flatfish',\n",
        "    'Sea spider',\n",
        "    'Worm'\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KA9vCCweOZH"
      },
      "source": [
        "With all the parameters and file paths set up, you can now point Detectron to the configurations using the `get_cfg()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC50iab9eIne"
      },
      "outputs": [],
      "source": [
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/retinanet_R_50_FPN_3x.yaml\"))\n",
        "cfg.merge_from_file(CONFIG_FILE)\n",
        "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = SCORE_THRESH\n",
        "cfg.MODEL.WEIGHTS = WEIGHT_FILE "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Akq8aB8ieik3"
      },
      "source": [
        "Load in all the model weights and set the thresholds. This actually instantiates the model in your workspace. The `model` object is what will ingest the images and return outputs for us to look at. \n",
        "\n",
        "⚠ *If this cell returns a* `RuntimeError: No CUDA GPUs are available` *you will need to update your settings. Click the Runtime dropdown menu, select \"Change runtime type\" and select GPU in the \"Hardware accelarator\" box. You will then need to rerun the detectron2 install via pip.*  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrtudCKeefcM"
      },
      "outputs": [],
      "source": [
        "model = build_model(cfg)                      # returns a torch.nn.Module\n",
        "checkpointer = DetectionCheckpointer(model)\n",
        "checkpointer.load(cfg.MODEL.WEIGHTS)          # This sets the weights to the pre-trained values dowloaded from Zenodo\n",
        "model.eval()                                  # Tell detectron that this model will only run inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HELDrDbTInnO"
      },
      "source": [
        "Before putting images through the network, you need to define some preprocessing steps. At training time, you might set up a series of random affine transformations to help guard against overfitting. Since this network is already trained, we just need to resize time images to a standard dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTIOGKlQI7nX"
      },
      "outputs": [],
      "source": [
        "aug = T.ResizeShortestEdge(\n",
        "  short_edge_length=[cfg.INPUT.MIN_SIZE_TEST], \n",
        "  max_size=cfg.INPUT.MAX_SIZE_TEST, \n",
        "  sample_style=\"choice\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbOymMvZJLH2"
      },
      "source": [
        "Finally, we need to set up an extra NMS layer since by default `detectron2` models only do intra-class comparisions between bounding boxes. We need to do another NMS run between classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAomTUA2O2s2"
      },
      "outputs": [],
      "source": [
        "post_process_nms = torchvision.ops.nms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnL6nUpqYyzu"
      },
      "source": [
        "We'll need to grab a random (or not so random) image to run through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRm3aVQwGbab"
      },
      "outputs": [],
      "source": [
        "from fathomnet.api import boundingboxes, images\n",
        "\n",
        "# Get a list of all concepts\n",
        "all_concepts = boundingboxes.find_concepts()\n",
        "\n",
        "# Pick one at random, or set one yourself, e.g.:\n",
        "# concept = 'Chionoecetes tanneri'\n",
        "concept = all_concepts[random.randrange(len(all_concepts))]\n",
        "\n",
        "# List the images of the concept in FathomNet\n",
        "concept_images = images.find_by_concept(concept)\n",
        "\n",
        "print(f'{len(concept_images)} images of {concept}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxMyTgddZ_Ro"
      },
      "source": [
        "Finally, you have everything loaded up to run the image through the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piXtsgAbAwlI"
      },
      "outputs": [],
      "source": [
        "# Pick a random image\n",
        "image = concept_images[random.randrange(len(concept_images))]\n",
        "\n",
        "# Fetch the image\n",
        "im = np.array(Image.open(requests.get(image.url, stream=True).raw))\n",
        "\n",
        "im_height,im_width,_ = im.shape  # Grab the image dimensions\n",
        "\n",
        "# Use detectron's visualization tool to plot the bounding boxes\n",
        "v_inf = Visualizer(\n",
        "  im,\n",
        "  metadata=fathomnet_metadata, \n",
        "  scale=1.0, \n",
        "  instance_mode=ColorMode.IMAGE\n",
        ")\n",
        "\n",
        "# Transform the image in the desired input shape\n",
        "im_transformed = aug.get_transform(im).apply_image(im)\n",
        "\n",
        "# Actually crank it through the model\n",
        "with torch.no_grad():\n",
        "  im_tensor = torch.as_tensor(im_transformed.astype('float32').transpose(2, 0, 1))\n",
        "  model_outputs = model([{\n",
        "    'image': im_tensor, \n",
        "    'height': im_height, \n",
        "    'width': im_width\n",
        "  }])[0]\n",
        "\n",
        "# Run the second stage NMS to ensure limited interclass overlap\n",
        "model_outputs['instances'] = model_outputs['instances'][\n",
        "  post_process_nms(\n",
        "    model_outputs['instances'].pred_boxes.tensor, \n",
        "    model_outputs['instances'].scores, \n",
        "    NMS_THRESH\n",
        "  ).to('cpu').tolist()\n",
        "]\n",
        "\n",
        "# Use the visualization tool to plot the bounding boxes on top of the image\n",
        "out_inf_raw = v_inf.draw_instance_predictions(model_outputs[\"instances\"].to(\"cpu\"))\n",
        "out_pil = Image.fromarray(out_inf_raw.get_image())\n",
        "\n",
        "# Show it\n",
        "display(out_pil)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9_G5azl_OXC"
      },
      "source": [
        "## That's all, folks!\n",
        "\n",
        "At this point, you have\n",
        "1. Used the FathomNet Python API to pull down and visualize concepts, images, and ancillary data\n",
        "2. Downloaded images and bounding boxes locally (all that data is still in the notebook instance, in truth)\n",
        "3. Trained your own object detection model using FathomNet data\n",
        "4. Run a pre-trained model from the FathomNet model zoo\n",
        "\n",
        "We hope this notebook has helped you understand the FathomNet Python API. Thanks for attending the workshop! \n",
        "\n",
        "If you have any feedback or suggestions, please open an issue on the [fathomnet-py issues page](https://github.com/fathomnet/fathomnet-py/issues). We very much appreciate your thoughts."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FathomNet Python API Tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
