{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an object detector from FathomNet data using Detectron2\n",
    "In this notebook, we'll:\n",
    "1. Generate a dataset from FathomNet using the `fathomnet-generate` script\n",
    "2. Split it into train/test sets\n",
    "3. Train an object detection model (via Detectron2) on the train set\n",
    "4. Evaluate the model on the test set\n",
    "5. Visualize some predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, we need to install some relevant libraries and import them. We'll also define some constants that we'll use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fathomnet torch torchvision pillow 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from detectron2.model_zoo import get_config_file, get_checkpoint_url\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer, DefaultPredictor\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from PIL import Image\n",
    "from torch import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONCEPTS = [\n",
    "    'Aegina citrea',\n",
    "]\n",
    "\n",
    "FULL_DATASET_NAME = 'demo_dataset'\n",
    "TRAIN_DATASET_NAME = FULL_DATASET_NAME + '_train'\n",
    "TEST_DATASET_NAME = FULL_DATASET_NAME + '_test'\n",
    "\n",
    "DATASET_DIR = Path(FULL_DATASET_NAME)\n",
    "IMAGE_DIR = DATASET_DIR / 'images'\n",
    "\n",
    "# Stringify for use in shell commands\n",
    "CONCEPTS_STR = \"'\" + ','.join(CONCEPTS) + \"'\"\n",
    "DATASET_DIR_STR = \"'\" + str(DATASET_DIR) + \"'\"\n",
    "IMAGE_DIR_STR = \"'\" + str(IMAGE_DIR) + \"'\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate a dataset from FathomNet\n",
    "This command will invoke the `fathomnet-generate` script to query for images and annotations from FathomNet. It will download the images and annotations (formatted as COCO JSON) into `DATASET_DIR` defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fathomnet-generate --format coco --concepts $CONCEPTS_STR --img-download $IMAGE_DIR_STR --output $DATASET_DIR_STR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split the dataset into train/test sets\n",
    "Detectron2 uses an internal registry of datasets. We'll load our original COCO dataset into the registry, and then split it into train/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset(s) are already registered, remove them so they can be re-registered\n",
    "for name in (FULL_DATASET_NAME, TRAIN_DATASET_NAME, TEST_DATASET_NAME):\n",
    "    if name in DatasetCatalog.list():\n",
    "        DatasetCatalog.remove(name)\n",
    "    if name in MetadataCatalog.list():\n",
    "        MetadataCatalog.remove(name)\n",
    "\n",
    "# Register the full dataset\n",
    "register_coco_instances(FULL_DATASET_NAME, {}, str(DATASET_DIR / 'dataset.json'), str(IMAGE_DIR))\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "dataset_dicts = DatasetCatalog.get(FULL_DATASET_NAME)\n",
    "split = int(len(dataset_dicts) * 0.8)\n",
    "random.shuffle(dataset_dicts)  # Shuffle the dataset to ensure randomness\n",
    "train_dicts = dataset_dicts[:split]\n",
    "test_dicts = dataset_dicts[split:]\n",
    "\n",
    "# Register the train and test sets\n",
    "full_metadata = MetadataCatalog.get(FULL_DATASET_NAME)\n",
    "DatasetCatalog.register(TRAIN_DATASET_NAME, lambda: train_dicts)\n",
    "MetadataCatalog.get(TRAIN_DATASET_NAME).set(thing_classes=full_metadata.thing_classes)\n",
    "DatasetCatalog.register(TEST_DATASET_NAME, lambda: test_dicts)\n",
    "MetadataCatalog.get(TEST_DATASET_NAME).set(thing_classes=full_metadata.thing_classes)\n",
    "\n",
    "train_metadata = MetadataCatalog.get(TRAIN_DATASET_NAME)\n",
    "test_metadata = MetadataCatalog.get(TEST_DATASET_NAME)\n",
    "\n",
    "print(f'Train dataset has {len(train_dicts)} images')\n",
    "print(f'Test dataset has {len(test_dicts)} images')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train an object detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Configure\n",
    "Now we'll set some configuration options for the training. We'll use a pre-trained Faster R-CNN model and train it for 200 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'COCO-Detection/retinanet_R_50_FPN_3x.yaml'                          # The base network to use\n",
    "\n",
    "# Define the training configuration\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(get_config_file(BACKBONE))                                  # Retrieve the base network configuration\n",
    "cfg.DATASETS.TRAIN = (TRAIN_DATASET_NAME,)                                      # Tell it what to use for training\n",
    "cfg.DATASETS.TEST = (TEST_DATASET_NAME,)                                         # Define test data (we are leaving this blank for speed)\n",
    "cfg.DATALOADER.NUM_WORKERS = 2                                                  # The number of threads to start for moving images to the GPU\n",
    "cfg.MODEL.WEIGHTS = get_checkpoint_url(BACKBONE)                                # Retrieve the weights for the desired base network\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2                                                    # How many images to give the GPU at a time. Make this bigger if you have a more powerful card\n",
    "cfg.SOLVER.BASE_LR = 0.0025                                                     # How much to move weights during backpropagation\n",
    "cfg.SOLVER.MAX_ITER = 200                                                       # How many times to run the training images through the network\n",
    "cfg.SOLVER.STEPS = []                                                           # When to change the learning rate. We are not making adjustements for this small training\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = len(train_metadata.thing_classes)             # The number of output classes\n",
    "\n",
    "# Create the output directory\n",
    "Path(cfg.OUTPUT_DIR).mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training\n",
    "Now we'll train the model. This can take a while, so we'll save the model to disk after training, so we can later load it and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up memory\n",
    "gc.collect()\n",
    "cuda.empty_cache()\n",
    "\n",
    "# Spin up a trainer\n",
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "\n",
    "# Train! This may take a while, depending on the configuration\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate the model\n",
    "Now that we have a trained object detector, let's evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "evaluator = COCOEvaluator(TEST_DATASET_NAME, cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
    "val_loader = build_detection_test_loader(cfg, TEST_DATASET_NAME)\n",
    "stats = inference_on_dataset(trainer.model, val_loader, evaluator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize some predictions\n",
    "Finally, we'll visualize some predictions from the model. We'll load the model from disk and run it on a few images from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model weights into the configuration\n",
    "cfg.MODEL.WEIGHTS = str(Path(cfg.OUTPUT_DIR) / 'model_final.pth')\n",
    "\n",
    "# Set the confidence threshold (predictions with confidence < will be omitted)\n",
    "cfg.MODEL.RETINANET.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "# Spin up a predictor with the provided config\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paths to the test images\n",
    "test_image_paths = [td['file_name'] for td in test_dicts]\n",
    "\n",
    "# Pick random images from the test set and run them through the model for display\n",
    "for image_path in random.sample(test_image_paths, 10):\n",
    "  image_pil = Image.open(image_path)\n",
    "  image = np.array(image_pil)\n",
    "\n",
    "  # Run the image through the model\n",
    "  outputs = predictor(image)\n",
    "\n",
    "  # Render the predictions on the image\n",
    "  visualizer = Visualizer(image, metadata=test_metadata, scale=0.5)\n",
    "  output_image = visualizer.draw_instance_predictions(outputs['instances'].to('cpu'))\n",
    "  output_image_pil = Image.fromarray(output_image.get_image())\n",
    "  \n",
    "  display(output_image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49e981309bf69e8dac3cb7ae1045727d5a914790cc54c5e45ce4e0e801321c63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
